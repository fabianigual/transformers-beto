{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación multiclase con Transformers\n",
    "\n",
    "Utilizando BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics, model_selection\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification,AutoModelForSequenceClassification, BertConfig, TrainingArguments, Trainer, EarlyStoppingCallback,  DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric,Dataset, DatasetDict, ClassLabel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12069 entries, 0 to 12068\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   codigo                12069 non-null  object\n",
      " 1   mercado_objetivo      12069 non-null  object\n",
      " 2   resumen_del_proyecto  12067 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 283.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"data/mercado-objetivo/mercados_procesados.xlsx\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>el ecommerce va en alza en chile y latam graci...</td>\n",
       "      <td>Telecomunicaciones y tecnologías de la informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>las características organolépticas es un facto...</td>\n",
       "      <td>Vitivinícola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la máquina de perforación blind hole posee un ...</td>\n",
       "      <td>Minería y metalurgia extractiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>actualmente un grupo muy reducido 15000 de los...</td>\n",
       "      <td>Telecomunicaciones y tecnologías de la informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>para lavar e higienizar prendas médicas hospit...</td>\n",
       "      <td>Química, caucho y plásticos (excepto industria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  el ecommerce va en alza en chile y latam graci...   \n",
       "1  las características organolépticas es un facto...   \n",
       "2  la máquina de perforación blind hole posee un ...   \n",
       "3  actualmente un grupo muy reducido 15000 de los...   \n",
       "4  para lavar e higienizar prendas médicas hospit...   \n",
       "\n",
       "                                               label  \n",
       "0  Telecomunicaciones y tecnologías de la informa...  \n",
       "1                                       Vitivinícola  \n",
       "2                    Minería y metalurgia extractiva  \n",
       "3  Telecomunicaciones y tecnologías de la informa...  \n",
       "4  Química, caucho y plásticos (excepto industria...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename({'resumen_del_proyecto':'text', 'mercado_objetivo':'label'}, axis=1)\n",
    "df = df[['text', 'label']]\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[6000:12000]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df.label = le.fit_transform(df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df.label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8), dpi=60)\n",
    "plt.hist(le.inverse_transform(df.label.values), bins=300)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "    epochs = 5 #2\n",
    "    batch_size = 64 #32\n",
    "    learning_rate = 2e-2  #2e-5 \n",
    "    train_batch_size = 64 \n",
    "    valid_batch_size = 64\n",
    "    max_len = 192 # 128\n",
    "    accumulation_steps = 1\n",
    "    test_size = 0.3 #03\n",
    "    num_labels = len(np.unique(df.label.values))\n",
    "    type = {'binary':'binary', 'multiclass':'weighted', 'micro':'micro', 'macro':'macro','multilabel':'samples'}\n",
    "    device =  torch.device('cpu')\n",
    "    dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model, use_cache=False, architectures='BertForSequenceClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = model_selection.train_test_split(df, test_size=args.test_size, stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Dataset.from_pandas(df_train)\n",
    "data_valid = Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bertdf = DatasetDict()\n",
    "Bertdf['train'] = data_train\n",
    "Bertdf['test'] = data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bertdf['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=args.max_len, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x0000022B6B36E550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.51ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.25ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_df= Bertdf.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_df['train']['input_ids'][3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_df['train'].features['label'] =  ClassLabel(num_classes=args.num_labels, names=le.classes_, names_file=None, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df.text:\n",
    "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "  token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(token_lens, kde=True)\n",
    "plt.xlim([0, 512])\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=args.max_len, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(args.model, num_labels= args.num_labels, classifier_dropout = args.dropout, \n",
    "                                    problem_type=\"single_label_classification\", use_cache=False)\n",
    "model = BertForSequenceClassification(config=config).to(args.device)  # remember to move to MPS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#batches_per_epoch = len(tokenized_df[\"train\"]) // args.batch_size\n",
    "total_train_steps = int(len(tokenized_df[\"train\"]) / args.batch_size * args.epochs)\n",
    "total_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=1e-2)\n",
    "num_training_steps = args.epochs * len(tokenized_df[\"train\"]) \n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "optimizers=optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average = args.type['micro'])[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BETOTrainer(Trainer):\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                # forward pass\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get(\"logits\")\n",
    "                # compute custom loss (suppose one has 3 labels with different weights)\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 330\n",
      "  1%|          | 3/330 [04:50<8:36:58, 94.86s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fabian.ortega\\OneDrive - corfo.cl\\transformers-beto\\torch-bert-multilabel.ipynb Celda 33\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=2'>3</a>\u001b[0m     overwrite_output_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=13'>14</a>\u001b[0m     load_best_model_at_end \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=16'>17</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=18'>19</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=25'>26</a>\u001b[0m     \u001b[39m#callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=26'>27</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fabian.ortega/OneDrive%20-%20corfo.cl/transformers-beto/torch-bert-multilabel.ipynb#ch0000027?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fabian.ortega\\Miniconda3\\envs\\beto\\lib\\site-packages\\transformers\\trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1408\u001b[0m )\n\u001b[1;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1414\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fabian.ortega\\Miniconda3\\envs\\beto\\lib\\site-packages\\transformers\\trainer.py:1651\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1649\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1651\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1653\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1654\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1655\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1656\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1657\u001b[0m ):\n\u001b[0;32m   1658\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\fabian.ortega\\Miniconda3\\envs\\beto\\lib\\site-packages\\transformers\\trainer.py:2363\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2363\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2365\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\fabian.ortega\\Miniconda3\\envs\\beto\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\fabian.ortega\\Miniconda3\\envs\\beto\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps = 50,\n",
    "    do_train=True,\n",
    "    learning_rate=args.learning_rate,\n",
    "    per_device_train_batch_size=args.train_batch_size,\n",
    "    per_device_eval_batch_size=args.valid_batch_size,\n",
    "    num_train_epochs=args.epochs,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_df[\"train\"],\n",
    "    eval_dataset=tokenized_df[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers= optimizers,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f'results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(tokenized_df['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs.predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs.predictions\n",
    "predictions[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment')\n",
    "\n",
    "class_names = le.classes_\n",
    "cm = metrics.confusion_matrix(outputs.label_ids, predictions)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('beto')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3908fa4646da46a86166e7cd23cd04ac285e24d64dbabf7ea71d68ac38528f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
